{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMNK3+R/t5LRoNv1JBgv+MP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ebhibhebhi/image_classifiers/blob/main/image_classifier_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Kaggle dataset\n",
        "\n",
        "!unzip -q \"/content/archive.zip\" -d \"/content/archive\""
      ],
      "metadata": {
        "id": "RCH_DwyGj0kU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/archive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-HbPZqekII9",
        "outputId": "68286e12-657a-4ed6-e9bb-508b7e66e0c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fish-vs-Cats\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEvU8rSEcJ2e",
        "outputId": "6801f089-d891-4d19-cd8f-2a0324bd0c81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "SRC_BASE = \"/content/archive/Fish-vs-Cats\"\n",
        "DST_BASE = \"/content/drive/MyDrive/Colab Notebooks/Programming PyTorch for DL/Fish-vs-Cats\"\n",
        "\n",
        "os.makedirs(DST_BASE, exist_ok=True)\n",
        "\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "    src = os.path.join(SRC_BASE, split)\n",
        "    dst = os.path.join(DST_BASE, split)\n",
        "\n",
        "    if os.path.exists(src):\n",
        "        shutil.copytree(src, dst, dirs_exist_ok=True)\n",
        "        print(f\"Copied {split} to Drive\")\n",
        "    else:\n",
        "        print(f\"Missing: {src}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQpZ40FQcJeR",
        "outputId": "e739a59e-cb30-4169-895d-dfbc5eebdd36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copied train to Drive\n",
            "Copied val to Drive\n",
            "Copied test to Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Removing corrupted files"
      ],
      "metadata": {
        "id": "YcWklkHBBpg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "ROOT = \"/content/archive/Fish-vs-Cats\"\n",
        "splits = [\"train\", \"val\", \"test\"]\n",
        "classes = [\"cat\", \"fish\"]\n",
        "\n",
        "bad_files = []\n",
        "\n",
        "# 1) Scan and collect bad images\n",
        "for split in splits:\n",
        "    for cls in classes:\n",
        "        folder = os.path.join(ROOT, split, cls)\n",
        "        for fname in os.listdir(folder):\n",
        "            path = os.path.join(folder, fname)\n",
        "            try:\n",
        "                with Image.open(path) as img:\n",
        "                    img.verify()  # checks image integrity\n",
        "            except Exception:\n",
        "                bad_files.append(path)\n",
        "\n",
        "# 2) Remove bad images\n",
        "for path in bad_files:\n",
        "    os.remove(path)\n",
        "\n",
        "print(f\"Removed {len(bad_files)} corrupted images.\\n\")\n",
        "\n",
        "# 3) Count remaining images\n",
        "print(\"Final dataset counts:\")\n",
        "total = 0\n",
        "for split in splits:\n",
        "    split_total = 0\n",
        "    print(f\"\\n{split.upper()}:\")\n",
        "    for cls in classes:\n",
        "        folder = os.path.join(ROOT, split, cls)\n",
        "        count = len([\n",
        "            f for f in os.listdir(folder)\n",
        "            if os.path.isfile(os.path.join(folder, f))\n",
        "        ])\n",
        "        split_total += count\n",
        "        print(f\"  {cls}: {count}\")\n",
        "    total += split_total\n",
        "    print(f\"  TOTAL: {split_total}\")\n",
        "\n",
        "print(f\"\\nOVERALL TOTAL IMAGES: {total}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guEwZXqPppIM",
        "outputId": "a449ae3f-7c08-46ff-cd2b-7034ffb8f31a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed 19 corrupted images.\n",
            "\n",
            "Final dataset counts:\n",
            "\n",
            "TRAIN:\n",
            "  cat: 417\n",
            "  fish: 384\n",
            "  TOTAL: 801\n",
            "\n",
            "VAL:\n",
            "  cat: 87\n",
            "  fish: 22\n",
            "  TOTAL: 109\n",
            "\n",
            "TEST:\n",
            "  cat: 91\n",
            "  fish: 69\n",
            "  TOTAL: 160\n",
            "\n",
            "OVERALL TOTAL IMAGES: 1070\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Balancing data"
      ],
      "metadata": {
        "id": "liW13DpUBuRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "ROOT = \"/content/archive/Fish-vs-Cats\"\n",
        "splits = [\"train\", \"val\", \"test\"]\n",
        "classes = [\"cat\", \"fish\"]\n",
        "\n",
        "SEED = 42\n",
        "DRY_RUN = False  # set to False to actually delete files\n",
        "\n",
        "random.seed(SEED)\n",
        "\n",
        "def list_files(folder):\n",
        "    return [\n",
        "        f for f in os.listdir(folder)\n",
        "        if os.path.isfile(os.path.join(folder, f))\n",
        "    ]\n",
        "\n",
        "def count_split(split):\n",
        "    counts = {}\n",
        "    for cls in classes:\n",
        "        folder = os.path.join(ROOT, split, cls)\n",
        "        counts[cls] = len(list_files(folder))\n",
        "    return counts\n",
        "\n",
        "print(\"BEFORE BALANCING:\")\n",
        "before_counts = {}\n",
        "for split in splits:\n",
        "    c = count_split(split)\n",
        "    before_counts[split] = c\n",
        "    print(f\"{split.upper()}: cat={c['cat']} fish={c['fish']} total={c['cat'] + c['fish']}\")\n",
        "\n",
        "print(\"\\nBALANCING PLAN (undersample to the min per split):\")\n",
        "to_delete = []\n",
        "\n",
        "for split in splits:\n",
        "    split_counts = before_counts[split]\n",
        "    target = min(split_counts[\"cat\"], split_counts[\"fish\"])\n",
        "    print(f\"\\n{split.upper()}: target per class = {target}\")\n",
        "\n",
        "    for cls in classes:\n",
        "        folder = os.path.join(ROOT, split, cls)\n",
        "        files = list_files(folder)\n",
        "        extra = len(files) - target\n",
        "\n",
        "        if extra > 0:\n",
        "            # randomly pick extra files to delete\n",
        "            delete_files = random.sample(files, extra)\n",
        "            delete_paths = [os.path.join(folder, f) for f in delete_files]\n",
        "            to_delete.extend(delete_paths)\n",
        "            print(f\"  {cls}: deleting {extra} files\")\n",
        "        else:\n",
        "            print(f\"  {cls}: deleting 0 files\")\n",
        "\n",
        "print(f\"\\nTOTAL FILES MARKED FOR DELETION: {len(to_delete)}\")\n",
        "\n",
        "if DRY_RUN:\n",
        "    print(\"\\nDRY_RUN=True so nothing was deleted.\")\n",
        "    print(\"If this looks right, set DRY_RUN=False and rerun.\")\n",
        "else:\n",
        "    deleted = 0\n",
        "    for path in to_delete:\n",
        "        if os.path.exists(path):\n",
        "            os.remove(path)\n",
        "            deleted += 1\n",
        "    print(f\"\\nDeleted {deleted} files.\")\n",
        "\n",
        "    print(\"\\nAFTER BALANCING:\")\n",
        "    total_after = 0\n",
        "    for split in splits:\n",
        "        c = count_split(split)\n",
        "        total_after += c[\"cat\"] + c[\"fish\"]\n",
        "        print(f\"{split.upper()}: cat={c['cat']} fish={c['fish']} total={c['cat'] + c['fish']}\")\n",
        "    print(f\"\\nOVERALL TOTAL IMAGES AFTER: {total_after}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uj3fo2cECKqU",
        "outputId": "986be84f-0ac0-4440-a7af-b94aabf9c8c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BEFORE BALANCING:\n",
            "TRAIN: cat=417 fish=384 total=801\n",
            "VAL: cat=87 fish=22 total=109\n",
            "TEST: cat=91 fish=69 total=160\n",
            "\n",
            "BALANCING PLAN (undersample to the min per split):\n",
            "\n",
            "TRAIN: target per class = 384\n",
            "  cat: deleting 33 files\n",
            "  fish: deleting 0 files\n",
            "\n",
            "VAL: target per class = 22\n",
            "  cat: deleting 65 files\n",
            "  fish: deleting 0 files\n",
            "\n",
            "TEST: target per class = 69\n",
            "  cat: deleting 22 files\n",
            "  fish: deleting 0 files\n",
            "\n",
            "TOTAL FILES MARKED FOR DELETION: 120\n",
            "\n",
            "Deleted 120 files.\n",
            "\n",
            "AFTER BALANCING:\n",
            "TRAIN: cat=384 fish=384 total=768\n",
            "VAL: cat=22 fish=22 total=44\n",
            "TEST: cat=69 fish=69 total=138\n",
            "\n",
            "OVERALL TOTAL IMAGES AFTER: 950\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preparing Datasets**"
      ],
      "metadata": {
        "id": "1rz0qkhsMOWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "ROOT = \"/content/archive/Fish-vs-Cats\"\n",
        "\n",
        "# Setting up training data\n",
        "train_data_path = f\"{ROOT}/train\"\n",
        "\n",
        "# what transform does is exactly what it sounds like, it is a machine that applies transformations to image data\n",
        "# When defining the object, you're basically choosing what transformations you want to apply to the data\n",
        "transform = transforms.Compose([transforms.Resize((64, 64)),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                     std=[0.229, 0.224, 0.225])\n",
        "                                ])\n",
        "# Here you are now actually creating the transformed dataset\n",
        "train_data = torchvision.datasets.ImageFolder(root=train_data_path, transform=transform)\n",
        "\n",
        "# Setting up validation data (compare accuracy against this set after each epoch)\n",
        "\n",
        "val_data_path = f\"{ROOT}/val\"\n",
        "val_data = torchvision.datasets.ImageFolder(root=val_data_path, transform=transform)\n",
        "\n",
        "# Setting up test data\n",
        "test_data_path = f\"{ROOT}/test\"\n",
        "test_data = torchvision.datasets.ImageFolder(root=test_data_path, transform=transform)"
      ],
      "metadata": {
        "id": "tFrncyQEKV4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Loaders**"
      ],
      "metadata": {
        "id": "2L7Jssd6MUDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from PIL import Image, ImageFile"
      ],
      "metadata": {
        "id": "0Kb1_zucde76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 64\n",
        "# These dataloaders will pass the data into our neural network\n",
        "train_data_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_data_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_data_loader = DataLoader(test_data, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "J2ZnUMX0MY-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Network**"
      ],
      "metadata": {
        "id": "eIdOfoK4Pmn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(12288, 1000)\n",
        "        self.fc2 = nn.Linear(1000, 100)\n",
        "        self.fc3 = nn.Linear(100, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 12288) # Converts our 3-D tensor into a 1-D tensor so that it can be fed to the first linear layer\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Be1dxclxPpN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simplenet = SimpleNet()"
      ],
      "metadata": {
        "id": "fslcQVn4dUz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Optimizer**"
      ],
      "metadata": {
        "id": "xt-6QVUnUVkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(simplenet.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "jk1heDGLUYSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "GEpyDcepVxOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch._higher_order_ops import out_dtype\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "simplenet.to(device)\n",
        "\n",
        "def train(model, optimizer, loss_fn, train_loader, val_loader, epochs=20, device=\"cpu\"):\n",
        "    for epoch in range(1, epochs+1):\n",
        "        training_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            inputs, target = batch\n",
        "            inputs = inputs.to(device)\n",
        "            target = target.to(device)\n",
        "            output = model(inputs)\n",
        "            loss = loss_fn(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            training_loss += loss.data.item() * inputs.size(0)\n",
        "        training_loss /= len(train_loader.dataset)\n",
        "\n",
        "        model.eval()\n",
        "        num_correct = 0\n",
        "        num_examples = 0\n",
        "        for batch in val_loader:\n",
        "            inputs, target = batch\n",
        "            inputs = inputs.to(device)\n",
        "            output = model(inputs)\n",
        "            target = target.to(device)\n",
        "            loss = loss_fn(output, target)\n",
        "            valid_loss += loss.data.item() *  inputs.size(0)\n",
        "            correct = torch.eq(torch.max(F.softmax(output), dim=1)[1], target).view(-1)\n",
        "\n",
        "            num_correct += torch.sum(correct).item()\n",
        "            num_examples += correct.shape[0]\n",
        "        valid_loss /= len(val_loader.dataset)\n",
        "\n",
        "        print(\"Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, accuracy = {:.2f}\".format(epoch, training_loss, valid_loss, num_correct / num_examples))"
      ],
      "metadata": {
        "id": "lIkThWfrVzgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(simplenet,\n",
        "      optimizer,\n",
        "      torch.nn.CrossEntropyLoss(),\n",
        "      train_data_loader,\n",
        "      val_data_loader,\n",
        "      epochs=100,\n",
        "      device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUny-wESeNFE",
        "outputId": "0b8baf8b-1e76-44e7-b9a2-65793510ffcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3371257371.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  correct = torch.eq(torch.max(F.softmax(output), dim=1)[1], target).view(-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training Loss: 1.29, Validation Loss: 0.90, accuracy = 0.60\n",
            "Epoch: 2, Training Loss: 0.55, Validation Loss: 0.52, accuracy = 0.72\n",
            "Epoch: 3, Training Loss: 0.43, Validation Loss: 0.68, accuracy = 0.77\n",
            "Epoch: 4, Training Loss: 0.32, Validation Loss: 0.59, accuracy = 0.81\n",
            "Epoch: 5, Training Loss: 0.29, Validation Loss: 0.48, accuracy = 0.79\n",
            "Epoch: 6, Training Loss: 0.23, Validation Loss: 0.55, accuracy = 0.77\n",
            "Epoch: 7, Training Loss: 0.21, Validation Loss: 0.49, accuracy = 0.74\n",
            "Epoch: 8, Training Loss: 0.14, Validation Loss: 0.51, accuracy = 0.77\n",
            "Epoch: 9, Training Loss: 0.12, Validation Loss: 0.53, accuracy = 0.74\n",
            "Epoch: 10, Training Loss: 0.08, Validation Loss: 0.88, accuracy = 0.74\n",
            "Epoch: 11, Training Loss: 0.06, Validation Loss: 0.70, accuracy = 0.74\n",
            "Epoch: 12, Training Loss: 0.04, Validation Loss: 0.78, accuracy = 0.74\n",
            "Epoch: 13, Training Loss: 0.03, Validation Loss: 0.74, accuracy = 0.74\n",
            "Epoch: 14, Training Loss: 0.02, Validation Loss: 0.80, accuracy = 0.74\n",
            "Epoch: 15, Training Loss: 0.02, Validation Loss: 0.85, accuracy = 0.72\n",
            "Epoch: 16, Training Loss: 0.01, Validation Loss: 0.78, accuracy = 0.77\n",
            "Epoch: 17, Training Loss: 0.01, Validation Loss: 0.85, accuracy = 0.79\n",
            "Epoch: 18, Training Loss: 0.01, Validation Loss: 0.85, accuracy = 0.77\n",
            "Epoch: 19, Training Loss: 0.01, Validation Loss: 0.81, accuracy = 0.74\n",
            "Epoch: 20, Training Loss: 0.01, Validation Loss: 0.81, accuracy = 0.77\n",
            "Epoch: 21, Training Loss: 0.00, Validation Loss: 0.88, accuracy = 0.77\n",
            "Epoch: 22, Training Loss: 0.00, Validation Loss: 0.84, accuracy = 0.79\n",
            "Epoch: 23, Training Loss: 0.00, Validation Loss: 0.87, accuracy = 0.77\n",
            "Epoch: 24, Training Loss: 0.00, Validation Loss: 0.89, accuracy = 0.77\n",
            "Epoch: 25, Training Loss: 0.00, Validation Loss: 0.91, accuracy = 0.77\n",
            "Epoch: 26, Training Loss: 0.00, Validation Loss: 0.92, accuracy = 0.77\n",
            "Epoch: 27, Training Loss: 0.00, Validation Loss: 0.93, accuracy = 0.77\n",
            "Epoch: 28, Training Loss: 0.00, Validation Loss: 0.93, accuracy = 0.77\n",
            "Epoch: 29, Training Loss: 0.00, Validation Loss: 0.95, accuracy = 0.77\n",
            "Epoch: 30, Training Loss: 0.00, Validation Loss: 0.95, accuracy = 0.77\n",
            "Epoch: 31, Training Loss: 0.00, Validation Loss: 0.96, accuracy = 0.77\n",
            "Epoch: 32, Training Loss: 0.00, Validation Loss: 0.97, accuracy = 0.77\n",
            "Epoch: 33, Training Loss: 0.00, Validation Loss: 0.98, accuracy = 0.77\n",
            "Epoch: 34, Training Loss: 0.00, Validation Loss: 0.99, accuracy = 0.77\n",
            "Epoch: 35, Training Loss: 0.00, Validation Loss: 0.99, accuracy = 0.77\n",
            "Epoch: 36, Training Loss: 0.00, Validation Loss: 0.99, accuracy = 0.77\n",
            "Epoch: 37, Training Loss: 0.00, Validation Loss: 0.99, accuracy = 0.77\n",
            "Epoch: 38, Training Loss: 0.00, Validation Loss: 0.99, accuracy = 0.77\n",
            "Epoch: 39, Training Loss: 0.00, Validation Loss: 1.00, accuracy = 0.77\n",
            "Epoch: 40, Training Loss: 0.00, Validation Loss: 1.00, accuracy = 0.77\n",
            "Epoch: 41, Training Loss: 0.00, Validation Loss: 1.00, accuracy = 0.77\n",
            "Epoch: 42, Training Loss: 0.00, Validation Loss: 1.02, accuracy = 0.77\n",
            "Epoch: 43, Training Loss: 0.00, Validation Loss: 1.01, accuracy = 0.77\n",
            "Epoch: 44, Training Loss: 0.00, Validation Loss: 1.02, accuracy = 0.77\n",
            "Epoch: 45, Training Loss: 0.00, Validation Loss: 1.02, accuracy = 0.77\n",
            "Epoch: 46, Training Loss: 0.00, Validation Loss: 1.03, accuracy = 0.77\n",
            "Epoch: 47, Training Loss: 0.00, Validation Loss: 1.05, accuracy = 0.77\n",
            "Epoch: 48, Training Loss: 0.00, Validation Loss: 1.09, accuracy = 0.77\n",
            "Epoch: 49, Training Loss: 0.00, Validation Loss: 1.07, accuracy = 0.77\n",
            "Epoch: 50, Training Loss: 0.00, Validation Loss: 1.05, accuracy = 0.77\n",
            "Epoch: 51, Training Loss: 0.00, Validation Loss: 1.04, accuracy = 0.77\n",
            "Epoch: 52, Training Loss: 0.00, Validation Loss: 1.04, accuracy = 0.77\n",
            "Epoch: 53, Training Loss: 0.00, Validation Loss: 1.04, accuracy = 0.77\n",
            "Epoch: 54, Training Loss: 0.00, Validation Loss: 1.06, accuracy = 0.77\n",
            "Epoch: 55, Training Loss: 0.00, Validation Loss: 1.05, accuracy = 0.77\n",
            "Epoch: 56, Training Loss: 0.00, Validation Loss: 1.06, accuracy = 0.77\n",
            "Epoch: 57, Training Loss: 0.00, Validation Loss: 1.06, accuracy = 0.77\n",
            "Epoch: 58, Training Loss: 0.00, Validation Loss: 1.06, accuracy = 0.77\n",
            "Epoch: 59, Training Loss: 0.00, Validation Loss: 1.06, accuracy = 0.77\n",
            "Epoch: 60, Training Loss: 0.00, Validation Loss: 1.07, accuracy = 0.77\n",
            "Epoch: 61, Training Loss: 0.00, Validation Loss: 1.07, accuracy = 0.77\n",
            "Epoch: 62, Training Loss: 0.00, Validation Loss: 1.07, accuracy = 0.77\n",
            "Epoch: 63, Training Loss: 0.00, Validation Loss: 1.07, accuracy = 0.77\n",
            "Epoch: 64, Training Loss: 0.00, Validation Loss: 1.08, accuracy = 0.77\n",
            "Epoch: 65, Training Loss: 0.00, Validation Loss: 1.08, accuracy = 0.77\n",
            "Epoch: 66, Training Loss: 0.00, Validation Loss: 1.08, accuracy = 0.77\n",
            "Epoch: 67, Training Loss: 0.00, Validation Loss: 1.09, accuracy = 0.77\n",
            "Epoch: 68, Training Loss: 0.00, Validation Loss: 1.08, accuracy = 0.77\n",
            "Epoch: 69, Training Loss: 0.00, Validation Loss: 1.09, accuracy = 0.77\n",
            "Epoch: 70, Training Loss: 0.00, Validation Loss: 1.09, accuracy = 0.77\n",
            "Epoch: 71, Training Loss: 0.00, Validation Loss: 1.09, accuracy = 0.77\n",
            "Epoch: 72, Training Loss: 0.00, Validation Loss: 1.09, accuracy = 0.77\n",
            "Epoch: 73, Training Loss: 0.00, Validation Loss: 1.10, accuracy = 0.77\n",
            "Epoch: 74, Training Loss: 0.00, Validation Loss: 1.10, accuracy = 0.77\n",
            "Epoch: 75, Training Loss: 0.00, Validation Loss: 1.10, accuracy = 0.77\n",
            "Epoch: 76, Training Loss: 0.00, Validation Loss: 1.10, accuracy = 0.77\n",
            "Epoch: 77, Training Loss: 0.00, Validation Loss: 1.10, accuracy = 0.77\n",
            "Epoch: 78, Training Loss: 0.00, Validation Loss: 1.11, accuracy = 0.77\n",
            "Epoch: 79, Training Loss: 0.00, Validation Loss: 1.11, accuracy = 0.74\n",
            "Epoch: 80, Training Loss: 0.00, Validation Loss: 1.11, accuracy = 0.74\n",
            "Epoch: 81, Training Loss: 0.00, Validation Loss: 1.11, accuracy = 0.74\n",
            "Epoch: 82, Training Loss: 0.00, Validation Loss: 1.12, accuracy = 0.74\n",
            "Epoch: 83, Training Loss: 0.00, Validation Loss: 1.12, accuracy = 0.74\n",
            "Epoch: 84, Training Loss: 0.00, Validation Loss: 1.12, accuracy = 0.74\n",
            "Epoch: 85, Training Loss: 0.00, Validation Loss: 1.12, accuracy = 0.74\n",
            "Epoch: 86, Training Loss: 0.00, Validation Loss: 1.14, accuracy = 0.74\n",
            "Epoch: 87, Training Loss: 0.00, Validation Loss: 1.16, accuracy = 0.74\n",
            "Epoch: 88, Training Loss: 0.00, Validation Loss: 1.14, accuracy = 0.77\n",
            "Epoch: 89, Training Loss: 0.00, Validation Loss: 1.23, accuracy = 0.79\n",
            "Epoch: 90, Training Loss: 0.00, Validation Loss: 1.24, accuracy = 0.79\n",
            "Epoch: 91, Training Loss: 0.00, Validation Loss: 1.17, accuracy = 0.77\n",
            "Epoch: 92, Training Loss: 0.00, Validation Loss: 1.13, accuracy = 0.77\n",
            "Epoch: 93, Training Loss: 0.00, Validation Loss: 1.13, accuracy = 0.77\n",
            "Epoch: 94, Training Loss: 0.00, Validation Loss: 1.13, accuracy = 0.77\n",
            "Epoch: 95, Training Loss: 0.00, Validation Loss: 1.14, accuracy = 0.77\n",
            "Epoch: 96, Training Loss: 0.00, Validation Loss: 1.14, accuracy = 0.77\n",
            "Epoch: 97, Training Loss: 0.00, Validation Loss: 1.14, accuracy = 0.77\n",
            "Epoch: 98, Training Loss: 0.00, Validation Loss: 1.14, accuracy = 0.77\n",
            "Epoch: 99, Training Loss: 0.00, Validation Loss: 1.15, accuracy = 0.77\n",
            "Epoch: 100, Training Loss: 0.00, Validation Loss: 1.15, accuracy = 0.77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Making Predictions**"
      ],
      "metadata": {
        "id": "u4KEIkOttn52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "labels = ['cat', 'fish']\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "img = Image.open(\"/content/archive/Fish-vs-Cats/test/fish/238656308_45fef30371.jpg\")\n",
        "img = transform(img)\n",
        "img = img.unsqueeze(0).to(device)\n",
        "# the unsqueeze adds a batch dimension. So if the initial shape of our image was:\n",
        "# (3, 64, 64)\n",
        "# unsqueezing made it (1, 3, 64, 64)\n",
        "# because our model expects batches, even if it is of size one\n",
        "\n",
        "prediction = simplenet(img) # Output is raw logits, just whatever tensor the NN spews out\n",
        "prediction = prediction.argmax() # finds the index of the largest logit\n",
        "print(labels[prediction])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WjkMlmitnYz",
        "outputId": "48a5c37f-b25d-4882-8e88-313ff7265e2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Saving the model**"
      ],
      "metadata": {
        "id": "0P_9CVel0G4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the model in pickle format:\n",
        "# torch.save(simplenet, \"/tmp/simplenet\")\n",
        "\n",
        "# and we can reload the model as:\n",
        "# simplenet = torch.load(\"/tmp/simplenet\")\n",
        "\n",
        "# This stores both the parameters and the structure of the model to a file, but this may be a problem\n",
        "# if you change the structure of the model at a later time\n",
        "\n",
        "# For this reason it's more common to save a model's state_dict instead, which is a standard Python dict\n",
        "# which contains the maps of each layers parameters in the model:\n",
        "\n",
        "# torch.save(model.state_dict(), PATH)\n",
        "\n",
        "# To restore, create an instance of the model and then use load_state_dict\n",
        "\n",
        "# simplenet = SimpleNet()\n",
        "# simplenet_state_dict = torch.load(\"/tmp/simplenet\")\n",
        "# simplenet.load_state_dict(simplenet_state_dict)"
      ],
      "metadata": {
        "id": "4ViIiOxvzemX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}